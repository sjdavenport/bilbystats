"""
Functions for performing API calls
"""
from typing import Optional, List, Union
import openai
from openai import OpenAI
import anthropic
from google import genai

import pandas as pd
import numpy as np
import bilbystats as bs

from importlib import resources
from dotenv import load_dotenv
import os

import numpy as np

# Use importlib to get a temporary path to the installed .env file
env_path = resources.files("bilbystats.defaults").joinpath("apikeys.env")

# Load environment variables from the .env file
load_dotenv(dotenv_path=env_path)


def read_api_key(api_key_name: str = "openai") -> str:
    """
    Read an API key from a specified file

    This function reads an API key from a text file located in the provided
    directory. The file is expected to be named using the format
    '{api_key_name}_api_key.txt'. The contents of the file are stripped of
    leading and trailing whitespace before being returned.

    ---------------------------------------------------------------------------
    ARGUMENTS:
    api_key_name : str, optional (default="openai")
        The base name of the API key file (without the '_api_key.txt' suffix).

    ---------------------------------------------------------------------------
    OUTPUT:
    api_key : str
        The API key read from the specified file.

    ---------------------------------------------------------------------------
    AUTHORS: Samuel Davenport
    ---------------------------------------------------------------------------
    """
    api_key = os.getenv(api_key_name.upper() + "_API_KEY")
    if api_key is None:
        raise ValueError("The " + api_key_name +
                         " API key is not stored and must be added to bilbystats/defaults/apikey.env")
    return api_key


def llm_api(content: str, instructions: str = "", model_name: str = "gpt-4o") -> str:
    """
    Unified interface to call different LLM APIs based on the specified model name.

    This function routes the request to the appropriate API handler (e.g., OpenAI or Claude) 
    depending on the selected model name. It supports models like GPT-4o, DeepSeek, Claude, etc.

    ---------------------------------------------------------------------------
    ARGUMENTS:
    content : str
        User-provided prompt or content for which a response is requested.
    instructions : str
        System-level instructions to guide the model's behavior and response generation.
    model_name : str, optional
        The identifier for the target model to be used. Supports values such as
        "deepseek", "gpt-4o", "gpt-4-mini", "claude", and "claude-3-7-sonnet-20250219".
        Defaults to "gpt-4o".

    ---------------------------------------------------------------------------
    OUTPUT:
    output : str
        The text response generated by the selected LLM API.

    ---------------------------------------------------------------------------
    AUTHORS: Samuel Davenport
    ---------------------------------------------------------------------------
    """
    if model_name in ("deepseek", "gpt-4o", "gpt-4o-mini"):
        output = bs.openai_api(content, instructions, model_name)
    elif model_name in ("claude", "claude-3-7-sonnet-20250219"):
        output = bs.claude_api(content, instructions, model_name)
    elif model_name in ("gemini", "gemini-2.0-flash"):
        output = bs.gemini_api(instructions + content, model_name)
    elif model_name in ("llama", "llama3.2", "deepseek-r1:7b", "ds"):
        output = bs.ollama(content, instructions, model_name)
    else:
        output = bs.ollama(content, instructions, model_name)

    return output


def openai_api(content: str, instructions: str = "", model_name: str = "gpt-4o") -> str:
    """
    Call the OpenAI API with custom instructions and content.

    This function sends a request to the OpenAI API using the specified model,
    along with provided instructions and user content. It retrieves and returns
    the response content generated by the model.

    ---------------------------------------------------------------------------
    ARGUMENTS:
    instructions : str
        The system or developer-level instructions that guide the model's behavior.
    content : str
        The main user-provided content or prompt to process.
    model_name : str, optional (default="gpt-4o")
        The name of the OpenAI model to use for generating the completion.
        Typically "gpt-4o" or "gpt-4o-mini".

    ---------------------------------------------------------------------------
    OUTPUT:
    output : str
        The generated response content from the OpenAI API.

    ---------------------------------------------------------------------------
    AUTHORS: Samuel Davenport
    ---------------------------------------------------------------------------
    """

    if model_name == "deepseek":
        model_name = "deepseek-chat"

    if model_name in ("gpt-4o", "gpt-4o-mini"):
        openai.api_key = read_api_key("openai")

        # Now you can use the OpenAI client
        client = openai
    elif model_name in ("deepseek-chat"):
        client = OpenAI(api_key=read_api_key("deepseek"),
                        base_url="https://api.deepseek.com")
    else:
        raise ValueError(f"Unsupported model_name: {model_name}")

    completion = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system",
                "content": instructions},
            {
                "role": "user",
                "content": content
            }
        ]
    )
    response = completion.choices[0].message
    output = response.content
    return output


def claude_api(content: str, instructions: str = "", model_name: str = "claude-3-7-sonnet-20250219", temperature: float = 1.0) -> str:
    """
    Call the Claude API to generate a response based on user input and system instructions.

    This function initializes the Claude API client, constructs a message payload with the given 
    instructions and content, and retrieves the model's response text.

    ---------------------------------------------------------------------------
    ARGUMENTS:
    instructions : str
        System-level instructions guiding the model's behavior and response style.
    content : str
        User-provided content or prompt for which a response is requested.
    model_name : str, optional
        The name of the Claude model to be used for generating the response.
        Defaults to "claude-3-7-sonnet-20250219".
    temperature : float, optional
        Temperature parameter for controlling randomness in generation.
        Defaults to 1.0.

    ---------------------------------------------------------------------------
    OUTPUT:
    output : str
        The text content of the model's response generated by Claude.

    ---------------------------------------------------------------------------
    AUTHORS: Samuel Davenport
    ---------------------------------------------------------------------------
    """
    if model_name in ("claude"):
        model_name = "claude-3-7-sonnet-20250219"

    key = bs.read_api_key("claude")
    client = anthropic.Anthropic(api_key=key)
    message = client.messages.create(
        model=model_name,
        max_tokens=1000,
        temperature=temperature,
        system=instructions,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": content
                    }
                ]
            }
        ]
    )
    output = message.content[0].text
    return output


def gemini_api(content: str, model_name: str = "gemini-2.0-flash") -> str:
    """
    Generate content using the Gemini API.

    This function sends a request to the Gemini API to generate content based on 
    the provided input. If the default model name "gemini" is used, it is replaced 
    with "gemini-2.0-flash" to ensure compatibility. The function returns the 
    generated text content.

    ---------------------------------------------------------------------------
    ARGUMENTS:
    content : str
        The input prompt or content to be sent to the Gemini API for generation.
    model_name : str, optional (default="gemini-2.0-flash")
        The name of the Gemini model to use for content generation. If "gemini" 
        is specified, it defaults to "gemini-2.0-flash". Other options are e.g.
        gemini-2.5-flash-preview-04-17 and gemini-2.0-flash-lite and 
        gemini-2.5-pro-preview-05-06. See 
        https://ai.google.dev/gemini-api/docs/models for a list of the different
        possible models.

    ---------------------------------------------------------------------------
    OUTPUT:
    output : str
        The generated text returned by the Gemini API.

    ---------------------------------------------------------------------------
    AUTHORS: Samuel Davenport
    ---------------------------------------------------------------------------
    """
    if model_name == "gemini":
        model_name = "gemini-2.0-flash"

    client = genai.Client(api_key=bs.read_api_key("gemini"))

    response = client.models.generate_content(
        model=model_name,
        contents=content
    )
    output = response.text

    return output


def translate(text: str, model_name: str = "gpt-4o", languageout: str = "English") -> str:
    """
    Translate input text into a specified language using a language model.

    This function sends the input text to a language model API with an 
    instruction prompt that requests translation into the desired output 
    language. The translation result is returned as plain text.

    ---------------------------------------------------------------------------
    ARGUMENTS:
    text : str
        The input text to be translated.
    model_name : str, optional (default="gpt-4o")
        The name of the language model to be used for translation.
    languageout : str, optional (default="English")
        The target language for translation.

    ---------------------------------------------------------------------------
    OUTPUT:
    translation : str
        The translated text in the target language.
    ---------------------------------------------------------------------------
    AUTHORS: Samuel Davenport
    ---------------------------------------------------------------------------
    """
    instructions = "You are a translator. Only return the " + \
        languageout+" translation. Do not add anything else."
    translation = bs.llm_api(text, instructions, model_name=model_name)
    return translation


def api2data(colname: str, promptloc: str, dataloc: str, saveloc: str,
             task: str = 'Classify the following text: ', model_name: str = "gpt-4o",
             labels: List[str] = ['Label: ', 'Explanation: '],
             names: List[str] = ['label', 'explanation'],
             lowercase: List[bool] = [True, False]) -> None:
    """
    Apply an LLM API to a dataset column and save results.

    This function reads a dataset from a Parquet file, applies a large language model (LLM) API 
    to each entry in a specified column, and extracts a label and explanation from the model's 
    response. The results are saved back into the dataset under dynamically generated column 
    names based on the model name, unless explicitly specified.

    ---------------------------------------------------------------------------
    ARGUMENTS:
    colname : str
        Name of the column in the dataset whose entries will be passed as input to the LLM.
    promptloc : str
        Path to the file containing instructions or prompts for the LLM.
    dataloc : str
        Path to the input Parquet file containing the dataset.
    saveloc : str
        Path where the resulting Parquet file with model outputs will be saved.
    task : str, optional (default='Classify the following text: ')
        Task prefix to prepend to each input text before sending to the LLM.
    model_name : str, optional (default="gpt-4o")
        Name of the LLM model to be used for inference.
    labels : List[str], optional
        List of label prefixes to search for in the model's response.
    names : List[str], optional
        List of column names where the extracted fields will be stored.
    lowercase : List[bool], optional
        List indicating whether to convert each extracted field to lowercase.

    ---------------------------------------------------------------------------
    OUTPUT:
    None
        The function saves the updated dataset with new label and explanation columns 
        to the specified output Parquet file but does not return any object.
    ---------------------------------------------------------------------------
    AUTHORS: Samuel Davenport
    ---------------------------------------------------------------------------
    """
    final_names = []
    for i in np.arange(len(names)):
        name = names[i]
        final_names.append(f"{model_name}_{name}")

    df = pd.read_parquet(dataloc)

    # Initialize new columns with NaN
    for name in final_names:
        df[name] = np.nan

    with open(promptloc, 'r') as file:
        instructions = file.read()

    for i in np.arange(len(df)):
        bs.loader(i, len(df))

        try:
            # Try applying the llm and getting an answer.
            output = bs.llm_api(
                task + df[colname][i], instructions, model_name)
            lines = output.strip().splitlines()

            for j in np.arange(len(names)):
                label = lines[j].replace(labels[j], "").strip()
                df.loc[i, final_names[j]] = label.lower()
        except:
            # If the answer was not in the correct format, print the output.
            # print(df[colname][i])
            print(output)

    for i in np.arange(len(names)):
        if lowercase[i]:
            df[final_names[j]] = df[final_names[j]].str.lower()

    df.to_parquet(saveloc)


def api2data_dep(colname: str, promptloc: str, dataloc: str, saveloc: str,
                 task: str = 'Classify the following text: ', model_name: str = "gpt-4o",
                 label_name: Optional[str] = None, explanation_name: Optional[str] = None) -> None:
    """
    Apply an LLM API to a dataset column and save results.

    This function reads a dataset from a Parquet file, applies a large language model (LLM) API 
    to each entry in a specified column, and extracts a label and explanation from the model's 
    response. The results are saved back into the dataset under dynamically generated column 
    names based on the model name, unless explicitly specified.

    ---------------------------------------------------------------------------
    ARGUMENTS:
    colname : str
        Name of the column in the dataset whose entries will be passed as input to the LLM.
    promptloc : str
        Path to the file containing instructions or prompts for the LLM.
    dataloc : str
        Path to the input Parquet file containing the dataset.
    saveloc : str
        Path where the resulting Parquet file with model outputs will be saved.
    task : str, optional (default='Classify the following text: ')
        Task prefix to prepend to each input text before sending to the LLM.
    model_name : str, optional (default="gpt-4o")
        Name of the LLM model to be used for inference.
    label_name : str, optional (default=None)
        Column name where the extracted label will be stored in the dataset.
        If None, defaults to "<model_name>_label".
    explanation_name : str, optional (default=None)
        Column name where the extracted explanation will be stored in the dataset.
        If None, defaults to "<model_name>_explanation".

    ---------------------------------------------------------------------------
    OUTPUT:
    None
        The function saves the updated dataset with new label and explanation columns 
        to the specified output Parquet file but does not return any object.
    ---------------------------------------------------------------------------
    AUTHORS: Samuel Davenport
    ---------------------------------------------------------------------------
    """
    if not label_name:
        label_name = model_name + '_label'

    if not explanation_name:
        explanation_name = model_name + '_explanation'

    df = pd.read_parquet(dataloc)

    # df[label_name] = pd.Series([pd.NA] * len(df), dtype='string')
    # df[explanation_name] = pd.Series([pd.NA] * len(df), dtype='string')
    df[label_name] = np.nan
    df[explanation_name] = np.nan

    with open(promptloc, 'r') as file:
        instructions = file.read()

    for i in np.arange(len(df)):
        bs.loader(i, len(df))

        try:
            # Try applying the llm and getting an answer.
            output = bs.llm_api(
                task + df[colname][i], instructions, model_name)
            lines = output.strip().splitlines()
            label = lines[0].replace("Label:", "").strip()
            explanation = lines[1].replace("Explanation:", "").strip()
            df.loc[i, label_name] = label.lower()
            df.loc[i, explanation_name] = explanation
        except:
            # If the answer was not in the correct format, print the output.
            # print(df[colname][i])
            print(output)

    df.to_parquet(saveloc)


def detect_sentiment(text: str, model_name: str = "gpt-4o") -> str:
    """
    Detect the sentiment of a given text using a specified language model.

    This function uses the OpenAI API to analyze the sentiment of the provided
    text. It classifies the sentiment as 'positive', 'neutral', or 'negative',
    and returns only the sentiment label.

    ---------------------------------------------------------------------------
    ARGUMENTS:
    text : str
        The text for which sentiment analysis is to be performed.
    model_name : str, optional (default="gpt-4o")
        The name of the OpenAI model used for sentiment analysis.

    ---------------------------------------------------------------------------
    OUTPUT:
    sentiment : str
        The sentiment classification of the input text ('positive',
        'neutral', or 'negative').

    ---------------------------------------------------------------------------
    AUTHORS: Samuel Davenport
    ---------------------------------------------------------------------------
    """
    # instructions = "You are an expert in sentiment analysis."
    # content = "For the following text return positive/neutral/negative depending on the sentiment of the text. Just give the sentiment as an answer. Text: "
    # content = content + text
    instructions = 'You are an expert in sentiment analysis. Given any input text, determine the overall sentiment and classify it as one of the following categories: positive, neutral, or negative. Respond with only one of these words — "positive", "neutral", or "negative" — and nothing else.'
    sentiment = llm_api(text, instructions, model_name=model_name)
    return sentiment
